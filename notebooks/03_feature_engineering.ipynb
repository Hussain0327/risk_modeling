{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Feature Engineering\n",
    "\n",
    "Create and encode features for modeling.\n",
    "\n",
    "**Steps**:\n",
    "\n",
    "1. Create financial ratios\n",
    "2. One-Hot encode categorical variables\n",
    "3. Label encode high-cardinality categoricals\n",
    "4. Prepare final feature set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Paths\n",
    "INTERIM_DATA = Path(\"../data/interim/lending_club_cleaned.parquet\")\n",
    "PROCESSED_DATA = Path(\"../data/processed\")\n",
    "PROCESSED_DATA.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data shape: (1345310, 70)\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned data\n",
    "df = pd.read_parquet(INTERIM_DATA)\n",
    "print(f\"Loaded data shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Financial Ratios\n",
    "\n",
    "Domain-specific features that are meaningful for credit risk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan_to_income: mean=0.213, max=0.500\n"
     ]
    }
   ],
   "source": [
    "# Loan-to-Income Ratio\n",
    "# How much of annual income is the loan amount?\n",
    "if \"loan_amnt\" in df.columns and \"annual_inc\" in df.columns:\n",
    "    df[\"loan_to_income\"] = df[\"loan_amnt\"] / df[\"annual_inc\"]\n",
    "    # Cap extreme values\n",
    "    df[\"loan_to_income\"] = df[\"loan_to_income\"].clip(\n",
    "        upper=df[\"loan_to_income\"].quantile(0.99)\n",
    "    )\n",
    "    print(\n",
    "        f\"loan_to_income: mean={df['loan_to_income'].mean():.3f}, max={df['loan_to_income'].max():.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "payment_to_income: mean=0.079\n"
     ]
    }
   ],
   "source": [
    "# Payment Burden (Annual Payment / Annual Income)\n",
    "# What percentage of income goes to loan payments?\n",
    "if \"installment\" in df.columns and \"annual_inc\" in df.columns:\n",
    "    df[\"payment_to_income\"] = (df[\"installment\"] * 12) / df[\"annual_inc\"]\n",
    "    df[\"payment_to_income\"] = df[\"payment_to_income\"].clip(\n",
    "        upper=df[\"payment_to_income\"].quantile(0.99)\n",
    "    )\n",
    "    print(f\"payment_to_income: mean={df['payment_to_income'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high_utilization: 14.6% of borrowers\n"
     ]
    }
   ],
   "source": [
    "# High Utilization Flag\n",
    "# Credit utilization > 80% is a risk factor\n",
    "if \"revol_util\" in df.columns:\n",
    "    df[\"high_utilization\"] = (df[\"revol_util\"] > 80).astype(int)\n",
    "    print(f\"high_utilization: {df['high_utilization'].mean():.1%} of borrowers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTI Risk distribution:\n",
      "dti_risk\n",
      "moderate     562485\n",
      "high         501731\n",
      "low          246179\n",
      "very_high     34915\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# DTI Risk Category\n",
    "if \"dti\" in df.columns:\n",
    "    df[\"dti_risk\"] = pd.cut(\n",
    "        df[\"dti\"],\n",
    "        bins=[-np.inf, 10, 20, 35, np.inf],\n",
    "        labels=[\"low\", \"moderate\", \"high\", \"very_high\"],\n",
    "    )\n",
    "    print(\"DTI Risk distribution:\")\n",
    "    print(df[\"dti_risk\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Income category distribution:\n",
      "income_category\n",
      "lower_middle    518592\n",
      "middle          477381\n",
      "upper_middle    222096\n",
      "low              98935\n",
      "high             27945\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Income Category\n",
    "if \"annual_inc\" in df.columns:\n",
    "    df[\"income_category\"] = pd.cut(\n",
    "        df[\"annual_inc\"],\n",
    "        bins=[0, 30000, 60000, 100000, 200000, np.inf],\n",
    "        labels=[\"low\", \"lower_middle\", \"middle\", \"upper_middle\", \"high\"],\n",
    "    )\n",
    "    print(\"Income category distribution:\")\n",
    "    print(df[\"income_category\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added log transformations for skewed features\n"
     ]
    }
   ],
   "source": [
    "# Log transformations for skewed distributions\n",
    "if \"annual_inc\" in df.columns:\n",
    "    df[\"log_annual_inc\"] = np.log1p(df[\"annual_inc\"])\n",
    "\n",
    "if \"revol_bal\" in df.columns:\n",
    "    df[\"log_revol_bal\"] = np.log1p(df[\"revol_bal\"])\n",
    "\n",
    "print(\"Added log transformations for skewed features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encode Categorical Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns (13):\n",
      "  term: 2 unique values\n",
      "  grade: 7 unique values\n",
      "  sub_grade: 35 unique values\n",
      "  emp_length: 12 unique values\n",
      "  home_ownership: 6 unique values\n",
      "  verification_status: 3 unique values\n",
      "  purpose: 14 unique values\n",
      "  earliest_cr_line: 739 unique values\n",
      "  initial_list_status: 2 unique values\n",
      "  application_type: 2 unique values\n",
      "  disbursement_method: 2 unique values\n",
      "  dti_risk: 4 unique values\n",
      "  income_category: 5 unique values\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "print(f\"Categorical columns ({len(categorical_cols)}):\")\n",
    "for col in categorical_cols:\n",
    "    n_unique = df[col].nunique()\n",
    "    print(f\"  {col}: {n_unique} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot encoding columns: ['term', 'grade', 'home_ownership', 'verification_status', 'purpose', 'application_type', 'initial_list_status', 'dti_risk', 'income_category']\n",
      "Shape after One-Hot encoding: (1345310, 104)\n"
     ]
    }
   ],
   "source": [
    "# One-Hot Encoding for low-cardinality categoricals\n",
    "low_card_cols = [\n",
    "    \"term\",\n",
    "    \"grade\",\n",
    "    \"home_ownership\",\n",
    "    \"verification_status\",\n",
    "    \"purpose\",\n",
    "    \"application_type\",\n",
    "    \"initial_list_status\",\n",
    "    \"dti_risk\",\n",
    "    \"income_category\",\n",
    "]\n",
    "\n",
    "# Filter to existing columns\n",
    "low_card_cols = [c for c in low_card_cols if c in df.columns]\n",
    "print(f\"One-Hot encoding columns: {low_card_cols}\")\n",
    "\n",
    "df_encoded = pd.get_dummies(df, columns=low_card_cols, drop_first=True)\n",
    "print(f\"Shape after One-Hot encoding: {df_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoded sub_grade: 35 values\n"
     ]
    }
   ],
   "source": [
    "# Label Encoding for high-cardinality categoricals (sub_grade)\n",
    "if \"sub_grade\" in df_encoded.columns:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[\"sub_grade_encoded\"] = le.fit_transform(\n",
    "        df_encoded[\"sub_grade\"].astype(str)\n",
    "    )\n",
    "    df_encoded = df_encoded.drop(columns=[\"sub_grade\"])\n",
    "    print(\n",
    "        f\"Label encoded sub_grade: {df_encoded['sub_grade_encoded'].nunique()} values\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping remaining object columns: ['emp_length', 'earliest_cr_line', 'disbursement_method']\n"
     ]
    }
   ],
   "source": [
    "# Drop remaining object columns (text that can't be encoded meaningfully)\n",
    "remaining_object_cols = df_encoded.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "if remaining_object_cols:\n",
    "    print(f\"Dropping remaining object columns: {remaining_object_cols}\")\n",
    "    df_encoded = df_encoded.drop(columns=remaining_object_cols, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original emp_length (we have emp_length_numeric)\n",
    "if \"emp_length\" in df_encoded.columns:\n",
    "    df_encoded = df_encoded.drop(columns=[\"emp_length\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Final Cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values\n"
     ]
    }
   ],
   "source": [
    "# Handle any remaining missing values\n",
    "missing_count = df_encoded.isnull().sum().sum()\n",
    "if missing_count > 0:\n",
    "    print(f\"Filling {missing_count} remaining missing values with median\")\n",
    "    df_encoded = df_encoded.fillna(df_encoded.median())\n",
    "else:\n",
    "    print(\"No missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle infinite values\n",
    "df_encoded = df_encoded.replace([np.inf, -np.inf], np.nan)\n",
    "df_encoded = df_encoded.fillna(df_encoded.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types:\n",
      "float64    62\n",
      "bool       36\n",
      "int64       3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Final data types check\n",
    "print(\"\\nData types:\")\n",
    "print(df_encoded.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FEATURE ENGINEERING SUMMARY\n",
      "==================================================\n",
      "Final shape: (1345310, 101)\n",
      "Number of features: 100\n",
      "Target: 'default'\n",
      "Default rate: 19.96%\n"
     ]
    }
   ],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Final shape: {df_encoded.shape}\")\n",
    "print(f\"Number of features: {df_encoded.shape[1] - 1}\")  # Exclude target\n",
    "print(f\"Target: 'default'\")\n",
    "print(f\"Default rate: {df_encoded['default'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features (100):\n",
      "  1. loan_amnt\n",
      "  2. int_rate\n",
      "  3. installment\n",
      "  4. annual_inc\n",
      "  5. dti\n",
      "  6. delinq_2yrs\n",
      "  7. fico_range_low\n",
      "  8. fico_range_high\n",
      "  9. inq_last_6mths\n",
      " 10. open_acc\n",
      " 11. pub_rec\n",
      " 12. revol_bal\n",
      " 13. revol_util\n",
      " 14. total_acc\n",
      " 15. last_fico_range_high\n",
      " 16. last_fico_range_low\n",
      " 17. collections_12_mths_ex_med\n",
      " 18. policy_code\n",
      " 19. acc_now_delinq\n",
      " 20. tot_coll_amt\n",
      " 21. tot_cur_bal\n",
      " 22. total_rev_hi_lim\n",
      " 23. acc_open_past_24mths\n",
      " 24. avg_cur_bal\n",
      " 25. bc_open_to_buy\n",
      " 26. bc_util\n",
      " 27. chargeoff_within_12_mths\n",
      " 28. delinq_amnt\n",
      " 29. mo_sin_old_il_acct\n",
      " 30. mo_sin_old_rev_tl_op\n",
      " 31. mo_sin_rcnt_rev_tl_op\n",
      " 32. mo_sin_rcnt_tl\n",
      " 33. mort_acc\n",
      " 34. mths_since_recent_bc\n",
      " 35. mths_since_recent_inq\n",
      " 36. num_accts_ever_120_pd\n",
      " 37. num_actv_bc_tl\n",
      " 38. num_actv_rev_tl\n",
      " 39. num_bc_sats\n",
      " 40. num_bc_tl\n",
      " 41. num_il_tl\n",
      " 42. num_op_rev_tl\n",
      " 43. num_rev_accts\n",
      " 44. num_rev_tl_bal_gt_0\n",
      " 45. num_sats\n",
      " 46. num_tl_120dpd_2m\n",
      " 47. num_tl_30dpd\n",
      " 48. num_tl_90g_dpd_24m\n",
      " 49. num_tl_op_past_12m\n",
      " 50. pct_tl_nvr_dlq\n",
      " 51. percent_bc_gt_75\n",
      " 52. pub_rec_bankruptcies\n",
      " 53. tax_liens\n",
      " 54. tot_hi_cred_lim\n",
      " 55. total_bal_ex_mort\n",
      " 56. total_bc_limit\n",
      " 57. total_il_high_credit_limit\n",
      " 58. emp_length_numeric\n",
      " 59. loan_to_income\n",
      " 60. payment_to_income\n",
      " 61. high_utilization\n",
      " 62. log_annual_inc\n",
      " 63. log_revol_bal\n",
      " 64. term_ 60 months\n",
      " 65. grade_B\n",
      " 66. grade_C\n",
      " 67. grade_D\n",
      " 68. grade_E\n",
      " 69. grade_F\n",
      " 70. grade_G\n",
      " 71. home_ownership_MORTGAGE\n",
      " 72. home_ownership_NONE\n",
      " 73. home_ownership_OTHER\n",
      " 74. home_ownership_OWN\n",
      " 75. home_ownership_RENT\n",
      " 76. verification_status_Source Verified\n",
      " 77. verification_status_Verified\n",
      " 78. purpose_credit_card\n",
      " 79. purpose_debt_consolidation\n",
      " 80. purpose_educational\n",
      " 81. purpose_home_improvement\n",
      " 82. purpose_house\n",
      " 83. purpose_major_purchase\n",
      " 84. purpose_medical\n",
      " 85. purpose_moving\n",
      " 86. purpose_other\n",
      " 87. purpose_renewable_energy\n",
      " 88. purpose_small_business\n",
      " 89. purpose_vacation\n",
      " 90. purpose_wedding\n",
      " 91. application_type_Joint App\n",
      " 92. initial_list_status_w\n",
      " 93. dti_risk_moderate\n",
      " 94. dti_risk_high\n",
      " 95. dti_risk_very_high\n",
      " 96. income_category_lower_middle\n",
      " 97. income_category_middle\n",
      " 98. income_category_upper_middle\n",
      " 99. income_category_high\n",
      "100. sub_grade_encoded\n"
     ]
    }
   ],
   "source": [
    "# List all features\n",
    "features = [c for c in df_encoded.columns if c != \"default\"]\n",
    "print(f\"\\nFeatures ({len(features)}):\")\n",
    "for i, feat in enumerate(features, 1):\n",
    "    print(f\"{i:3}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Processed Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: ../data/processed/lending_club_processed.parquet\n",
      "File size: 108.9 MB\n"
     ]
    }
   ],
   "source": [
    "# Save to processed folder\n",
    "output_path = PROCESSED_DATA / \"lending_club_processed.parquet\"\n",
    "df_encoded.to_parquet(output_path, index=False)\n",
    "\n",
    "import os\n",
    "\n",
    "file_size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
    "print(f\"Saved to: {output_path}\")\n",
    "print(f\"File size: {file_size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Proceed to `04_eda.ipynb` for:\n",
    "\n",
    "- Exploratory Data Analysis\n",
    "- Visualizations\n",
    "- Correlation analysis\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
