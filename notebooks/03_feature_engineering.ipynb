{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Feature Engineering\n",
    "\n",
    "Create and encode features for modeling.\n",
    "\n",
    "**Steps**:\n",
    "1. Create financial ratios\n",
    "2. One-Hot encode categorical variables\n",
    "3. Label encode high-cardinality categoricals\n",
    "4. Prepare final feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Paths\n",
    "INTERIM_DATA = Path(\"../data/interim/lending_club_cleaned.parquet\")\n",
    "PROCESSED_DATA = Path(\"../data/processed\")\n",
    "PROCESSED_DATA.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "df = pd.read_parquet(INTERIM_DATA)\n",
    "print(f\"Loaded data shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Financial Ratios\n",
    "\n",
    "Domain-specific features that are meaningful for credit risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loan-to-Income Ratio\n",
    "# How much of annual income is the loan amount?\n",
    "if 'loan_amnt' in df.columns and 'annual_inc' in df.columns:\n",
    "    df['loan_to_income'] = df['loan_amnt'] / df['annual_inc']\n",
    "    # Cap extreme values\n",
    "    df['loan_to_income'] = df['loan_to_income'].clip(upper=df['loan_to_income'].quantile(0.99))\n",
    "    print(f\"loan_to_income: mean={df['loan_to_income'].mean():.3f}, max={df['loan_to_income'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payment Burden (Annual Payment / Annual Income)\n",
    "# What percentage of income goes to loan payments?\n",
    "if 'installment' in df.columns and 'annual_inc' in df.columns:\n",
    "    df['payment_to_income'] = (df['installment'] * 12) / df['annual_inc']\n",
    "    df['payment_to_income'] = df['payment_to_income'].clip(upper=df['payment_to_income'].quantile(0.99))\n",
    "    print(f\"payment_to_income: mean={df['payment_to_income'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High Utilization Flag\n",
    "# Credit utilization > 80% is a risk factor\n",
    "if 'revol_util' in df.columns:\n",
    "    df['high_utilization'] = (df['revol_util'] > 80).astype(int)\n",
    "    print(f\"high_utilization: {df['high_utilization'].mean():.1%} of borrowers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DTI Risk Category\n",
    "if 'dti' in df.columns:\n",
    "    df['dti_risk'] = pd.cut(\n",
    "        df['dti'], \n",
    "        bins=[-np.inf, 10, 20, 35, np.inf],\n",
    "        labels=['low', 'moderate', 'high', 'very_high']\n",
    "    )\n",
    "    print(\"DTI Risk distribution:\")\n",
    "    print(df['dti_risk'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Income Category\n",
    "if 'annual_inc' in df.columns:\n",
    "    df['income_category'] = pd.cut(\n",
    "        df['annual_inc'],\n",
    "        bins=[0, 30000, 60000, 100000, 200000, np.inf],\n",
    "        labels=['low', 'lower_middle', 'middle', 'upper_middle', 'high']\n",
    "    )\n",
    "    print(\"Income category distribution:\")\n",
    "    print(df['income_category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transformations for skewed distributions\n",
    "if 'annual_inc' in df.columns:\n",
    "    df['log_annual_inc'] = np.log1p(df['annual_inc'])\n",
    "    \n",
    "if 'revol_bal' in df.columns:\n",
    "    df['log_revol_bal'] = np.log1p(df['revol_bal'])\n",
    "\n",
    "print(\"Added log transformations for skewed features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(f\"Categorical columns ({len(categorical_cols)}):\")\n",
    "for col in categorical_cols:\n",
    "    n_unique = df[col].nunique()\n",
    "    print(f\"  {col}: {n_unique} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding for low-cardinality categoricals\n",
    "low_card_cols = ['term', 'grade', 'home_ownership', 'verification_status', \n",
    "                 'purpose', 'application_type', 'initial_list_status',\n",
    "                 'dti_risk', 'income_category']\n",
    "\n",
    "# Filter to existing columns\n",
    "low_card_cols = [c for c in low_card_cols if c in df.columns]\n",
    "print(f\"One-Hot encoding columns: {low_card_cols}\")\n",
    "\n",
    "df_encoded = pd.get_dummies(df, columns=low_card_cols, drop_first=True)\n",
    "print(f\"Shape after One-Hot encoding: {df_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding for high-cardinality categoricals (sub_grade)\n",
    "if 'sub_grade' in df_encoded.columns:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded['sub_grade_encoded'] = le.fit_transform(df_encoded['sub_grade'].astype(str))\n",
    "    df_encoded = df_encoded.drop(columns=['sub_grade'])\n",
    "    print(f\"Label encoded sub_grade: {df_encoded['sub_grade_encoded'].nunique()} values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop remaining object columns (text that can't be encoded meaningfully)\n",
    "remaining_object_cols = df_encoded.select_dtypes(include=['object']).columns.tolist()\n",
    "if remaining_object_cols:\n",
    "    print(f\"Dropping remaining object columns: {remaining_object_cols}\")\n",
    "    df_encoded = df_encoded.drop(columns=remaining_object_cols, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original emp_length (we have emp_length_numeric)\n",
    "if 'emp_length' in df_encoded.columns:\n",
    "    df_encoded = df_encoded.drop(columns=['emp_length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Final Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle any remaining missing values\n",
    "missing_count = df_encoded.isnull().sum().sum()\n",
    "if missing_count > 0:\n",
    "    print(f\"Filling {missing_count} remaining missing values with median\")\n",
    "    df_encoded = df_encoded.fillna(df_encoded.median())\n",
    "else:\n",
    "    print(\"No missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle infinite values\n",
    "df_encoded = df_encoded.replace([np.inf, -np.inf], np.nan)\n",
    "df_encoded = df_encoded.fillna(df_encoded.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final data types check\n",
    "print(\"\\nData types:\")\n",
    "print(df_encoded.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Final shape: {df_encoded.shape}\")\n",
    "print(f\"Number of features: {df_encoded.shape[1] - 1}\")  # Exclude target\n",
    "print(f\"Target: 'default'\")\n",
    "print(f\"Default rate: {df_encoded['default'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all features\n",
    "features = [c for c in df_encoded.columns if c != 'default']\n",
    "print(f\"\\nFeatures ({len(features)}):\")\n",
    "for i, feat in enumerate(features, 1):\n",
    "    print(f\"{i:3}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to processed folder\n",
    "output_path = PROCESSED_DATA / \"lending_club_processed.parquet\"\n",
    "df_encoded.to_parquet(output_path, index=False)\n",
    "\n",
    "import os\n",
    "file_size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
    "print(f\"Saved to: {output_path}\")\n",
    "print(f\"File size: {file_size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Proceed to `04_eda.ipynb` for:\n",
    "- Exploratory Data Analysis\n",
    "- Visualizations\n",
    "- Correlation analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
