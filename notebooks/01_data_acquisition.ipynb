{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Data Acquisition\n",
    "\n",
    "Download the LendingClub dataset from Kaggle and perform initial exploration.\n",
    "\n",
    "**Dataset**: [LendingClub Loan Data](https://www.kaggle.com/datasets/wordsforthewise/lending-club)\n",
    "\n",
    "**Contents**:\n",
    "- Download data using kagglehub\n",
    "- Initial exploration\n",
    "- Save as parquet for faster reloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths\n",
    "DATA_DIR = Path(\"../data/raw\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset\n",
    "\n",
    "Using kagglehub for programmatic download. Requires Kaggle API credentials.\n",
    "\n",
    "**Setup**: Create `~/.kaggle/kaggle.json` with your API key from https://www.kaggle.com/settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download LendingClub dataset\n",
    "path = kagglehub.dataset_download(\"wordsforthewise/lending-club\")\n",
    "print(f\"Dataset downloaded to: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List downloaded files\n",
    "import os\n",
    "files = os.listdir(path)\n",
    "print(\"Downloaded files:\")\n",
    "for f in files:\n",
    "    size_mb = os.path.getsize(os.path.join(path, f)) / (1024 * 1024)\n",
    "    print(f\"  - {f} ({size_mb:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the accepted loans dataset (the main one)\n",
    "# Try different possible filenames\n",
    "possible_files = [\n",
    "    \"accepted_2007_to_2018Q4.csv.gz\",\n",
    "    \"accepted_2007_to_2018Q4.csv\",\n",
    "    \"lending_club_loan_two.csv\"\n",
    "]\n",
    "\n",
    "df = None\n",
    "for filename in possible_files:\n",
    "    filepath = os.path.join(path, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"Loading {filename}...\")\n",
    "        df = pd.read_csv(filepath, low_memory=False)\n",
    "        break\n",
    "\n",
    "if df is None:\n",
    "    # Fallback: load any CSV file\n",
    "    csv_files = [f for f in files if f.endswith('.csv') or f.endswith('.csv.gz')]\n",
    "    if csv_files:\n",
    "        filepath = os.path.join(path, csv_files[0])\n",
    "        print(f\"Loading {csv_files[0]}...\")\n",
    "        df = pd.read_csv(filepath, low_memory=False)\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape[0]:,} rows x {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info\n",
    "print(\"Column names:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and memory usage\n",
    "df.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Variable: Loan Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target variable distribution\n",
    "print(\"Loan Status Distribution:\")\n",
    "print(\"=\" * 50)\n",
    "status_counts = df['loan_status'].value_counts()\n",
    "status_pct = df['loan_status'].value_counts(normalize=True) * 100\n",
    "\n",
    "for status in status_counts.index:\n",
    "    print(f\"{status:30} {status_counts[status]:>10,} ({status_pct[status]:>5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For binary classification, we'll use:\n",
    "# - Fully Paid = 0 (good loan)\n",
    "# - Charged Off = 1 (default)\n",
    "# We'll filter out ambiguous statuses in the cleaning notebook\n",
    "\n",
    "binary_statuses = ['Fully Paid', 'Charged Off']\n",
    "binary_count = df[df['loan_status'].isin(binary_statuses)].shape[0]\n",
    "print(f\"\\nRows with clear outcomes (Fully Paid/Charged Off): {binary_count:,}\")\n",
    "print(f\"Percentage of total: {binary_count/len(df)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Features Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key features we'll use\n",
    "key_features = [\n",
    "    'loan_amnt', 'term', 'int_rate', 'installment', 'grade', 'sub_grade',\n",
    "    'emp_length', 'home_ownership', 'annual_inc', 'verification_status',\n",
    "    'purpose', 'dti', 'open_acc', 'revol_bal', 'revol_util', 'total_acc'\n",
    "]\n",
    "\n",
    "existing_features = [f for f in key_features if f in df.columns]\n",
    "print(f\"Key features available: {len(existing_features)}/{len(key_features)}\")\n",
    "print(\"\\nPreview of key features:\")\n",
    "df[existing_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values in key features\n",
    "print(\"Missing values in key features:\")\n",
    "print(\"=\" * 50)\n",
    "missing = df[existing_features].isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({'Missing': missing, 'Pct': missing_pct})\n",
    "print(missing_df[missing_df['Missing'] > 0].sort_values('Pct', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Raw Data\n",
    "\n",
    "Save as parquet for faster loading in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as parquet\n",
    "output_path = DATA_DIR / \"lending_club_raw.parquet\"\n",
    "df.to_parquet(output_path, index=False)\n",
    "\n",
    "file_size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
    "print(f\"Saved to: {output_path}\")\n",
    "print(f\"File size: {file_size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify saved data\n",
    "df_verify = pd.read_parquet(output_path)\n",
    "print(f\"Verification: {df_verify.shape} == {df.shape} ? {df_verify.shape == df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Proceed to `02_data_cleaning.ipynb` to:\n",
    "- Filter to binary outcomes\n",
    "- Handle missing values\n",
    "- Remove data leakage columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
